# 算法
[TOC]

## 一. KNN算法
1. 算法实现的目的：分类和回归

2. 什么是分类：定性输出为分类，或者说离散变量的预测


3. 什么是回归：定量输出为回归，或者说连续变量的预测

4. 回归和分类的区别：在于输出变量的区别

5. 举个例子：
预测明天的气温是多少度，这是一个回归任务；
预测明天是阴、晴还是雨，就是一个分类任务。


6. 算法怎么实现：

    - 有一个数据集，每条数据都对应了类别
    - 新数据的每个特征都和数据集的每条数据的特征做对比
        - 计算新数据语数据集中每条数据的距离
        - 对所求得的距离进行排序，越小表明越相似
        - 取的前k个样本数据集的分类标签
        
    通俗的说：获取一个样本集，对于新输入的实例，找到样本中对新实例最近的k个实例，这k个实例是什么分类，那新实例就是什么分类。
    
7.  算法数据监督学习还是非监督学习：监督学习

8. 什么是监督学习和非监督学习

## 二、决策树

决策树可以用来分类和回归，本文只讨论用于分类的决策树。

什么是分类决策树：对实例进行分类的树状结构，由节点和有向边组成。节点由内部结点和叶结点组成。内部节点表示特征或者属性，叶节点表示分类。

什么是熵：熵越高越混乱，越低越有序

熵的计算步骤：

- 用循环，找出数据集的所有标签
- 用循环算出每一个标签的概率p(x)
- 通过p(x)算出信息熵

增益的计算步骤：

信息增益=E(x)-E(x1)P(x1)-E(x2)P(x2)

算法的实现步骤：

- 循环遍历每个特征
- 通过遍历到的特征划分出数据集
- 计算数据集的殇和增益
- 求出增益最好的特征
- 剔除该特征，继续第一步

## 三、朴素贝叶斯



应用场景:对邮件进行分类

前提：特征间相互独立

算法实现步骤：

该类别下出现该特征的概率，通过朴素贝叶斯公式，求出该特征下为该类别的概率。

## 四、logistic回归

**这个我搞不懂！！！！！！！！！！！！**

逻辑回归

这个回归是怎么实现的：

- 通过logistic函数

- logistic函数需要z参数
- z参数需要最佳系数
- 最佳系数需要由梯度上升法求出
- 

